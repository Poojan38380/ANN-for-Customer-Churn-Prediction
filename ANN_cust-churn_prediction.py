# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1l6FMkaGt9hgjC-XhsJkxPPfTWsWykaHO
"""

!pip install tensorflow

import tensorflow as tf
print(tf.__version__)

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

data = pd.read_csv("file.csv")
data.head()

X = data.iloc[:,3:13]
y = data.iloc[:,13]

geo=pd.get_dummies(X['Geography'],drop_first="true")
gender=pd.get_dummies(X['Gender'],drop_first="true")

X=X.drop(['Geography','Gender'],axis=1)

X = pd.concat([X,geo,gender],axis=1)

from sklearn.model_selection import train_test_split

Xtrain,Xtest,ytrain,ytest= train_test_split(X,y,test_size=0.2,random_state=0)

from sklearn.preprocessing import StandardScaler
sc=StandardScaler()
Xtrain = sc.fit_transform(Xtrain)
Xtest = sc.transform(Xtest)

# Creating the ANN

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense,ReLU,PReLU,LeakyReLU,ELU,Dropout

classifier = Sequential()

Xtrain.shape

#input layer
classifier.add(Dense(units = 11,activation = 'relu'))
#can add dropout by classifier.add(Dropout(number))

#1st Hidden layer
classifier.add(Dense(units = 7,activation = 'relu'))

#2nd Hidden layer
classifier.add(Dense(units = 5,activation = 'relu'))

#output layer
classifier.add(Dense(units = 1,activation = 'sigmoid'))

classifier.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])

#by default the learning rate is 0.001 but if the user wants to change it...
#import tensorflow
#opt =  tensorflow.keras.optimizers.Adam(learning_rate = "given number")
# replace the above line "classifier.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])" with "classifier.compile(optimizer='opt',loss='binary_crossentropy',metrics=['accuracy'])"

#early stopping
import tensorflow as tf
early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor="val_loss",
    min_delta=0.001,
    patience=20,
    verbose=1,
    mode="auto",
    baseline=None,
    restore_best_weights=False,
    start_from_epoch=0,
)

#training and validating the model
model_history = classifier.fit(Xtrain,ytrain,validation_split=0.33, batch_size= 10, epochs=1000,callbacks=early_stopping)

model_history.history.keys()

#accuracy plot
plt.plot(model_history.history['accuracy'])
plt.plot(model_history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train','val_test'],loc='upper left')

#loss plot
plt.plot(model_history.history['loss'])
plt.plot(model_history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train','val_test'],loc='upper left')

#evaluating the model
ypred = classifier.predict(Xtest)
ypred = (ypred>=0.5)

#confusion matrix and accuracy score
from sklearn.metrics import confusion_matrix,accuracy_score
cm = confusion_matrix(ytest,ypred)

cm

from sklearn.metrics import accuracy_score

score = accuracy_score(ypred,ytest)
score

#to see the weights of each layer
classifier.get_weights()

